{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDACS Dataset - Complete Tutorial\n",
    "\n",
    "This notebook demonstrates all features of the Deep Drawing and Cutting Simulations (DDACS) Dataset package.\n",
    "\n",
    "## Installation\n",
    "\n",
    "If you haven't installed the package yet, run:\n",
    "```bash\n",
    "pip install \"git+https://github.com/BaumSebastian/Deep-Drawing-and-Cutting-Simulations-Dataset.git[examples]\"\n",
    "```\n",
    "\n",
    "## Topics Covered\n",
    "\n",
    "1. Dataset Loading and Exploration\n",
    "2. Different Access Patterns (PyTorch, Iterator, Generator)\n",
    "3. Performance Comparisons\n",
    "4. Data Visualizations\n",
    "5. Machine Learning Workflow Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Import DDACS package\n",
    "from ddacs import SimulationDataset, SimulationIterator, iter_simulations\n",
    "\n",
    "# PyTorch imports (optional)\n",
    "try:\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "    TORCH_AVAILABLE = True\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"PyTorch not available - some examples will be skipped\")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Setup\n",
    "\n",
    "First, make sure you have downloaded the dataset using the `darus-download` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset exists\n",
    "data_dir = Path(\"./data\")\n",
    "h5_dir = data_dir / \"h5\"\n",
    "metadata_file = data_dir / \"metadata.csv\"\n",
    "\n",
    "if not data_dir.exists():\n",
    "    print(\"❌ Dataset not found!\")\n",
    "    print(\"Please download the dataset first:\")\n",
    "    print('darus-download --url \"https://darus.uni-stuttgart.de/dataset.xhtml?persistentId=doi:10.18419/DARUS-4801\" --path \"./data\"')\n",
    "elif not h5_dir.exists() or not metadata_file.exists():\n",
    "    print(\"❌ Dataset incomplete!\")\n",
    "    print(f\"Missing: {h5_dir if not h5_dir.exists() else metadata_file}\")\n",
    "else:\n",
    "    n_h5_files = len(list(h5_dir.glob(\"*.h5\")))\n",
    "    dataset_size = sum(f.stat().st_size for f in data_dir.rglob(\"*\") if f.is_file())\n",
    "    print(f\"✅ Dataset found!\")\n",
    "    print(f\"   H5 files: {n_h5_files}\")\n",
    "    print(f\"   Total size: {dataset_size / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading and Exploration\n",
    "\n",
    "Let's explore the dataset using different access methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with PyTorch-compatible class\n",
    "try:\n",
    "    dataset = SimulationDataset(data_dir, \"h5\")\n",
    "    print(dataset)\n",
    "    print(f\"\\nDataset length: {len(dataset)}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore metadata\n",
    "if metadata_file.exists():\n",
    "    metadata = pd.read_csv(metadata_file)\n",
    "    print(\"Metadata shape:\", metadata.shape)\n",
    "    print(\"\\nColumns:\", list(metadata.columns))\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    display(metadata.head())\n",
    "    \n",
    "    print(\"\\nMetadata statistics:\")\n",
    "    display(metadata.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Access Pattern Comparison\n",
    "\n",
    "Compare different ways to access the dataset data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: PyTorch Dataset (for ML training)\n",
    "print(\"=== PyTorch Dataset ===\")\n",
    "try:\n",
    "    dataset = SimulationDataset(data_dir, \"h5\")\n",
    "    sim_id, metadata_vals, h5_path = dataset[0]\n",
    "    print(f\"Sample ID: {sim_id}\")\n",
    "    print(f\"Metadata shape: {metadata_vals.shape}\")\n",
    "    print(f\"H5 file: {h5_path.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Lightweight Iterator (no PyTorch dependency)\n",
    "print(\"=== Lightweight Iterator ===\")\n",
    "try:\n",
    "    iterator = SimulationIterator(data_dir, \"h5\")\n",
    "    print(iterator)\n",
    "    \n",
    "    # Get first simulation\n",
    "    sim_id, metadata_vals, h5_path = next(iter(iterator))\n",
    "    print(f\"\\nSample ID: {sim_id}\")\n",
    "    print(f\"Metadata shape: {metadata_vals.shape}\")\n",
    "    \n",
    "    # Sample random simulations\n",
    "    print(\"\\nRandom samples:\")\n",
    "    for i, (sim_id, metadata_vals, h5_path) in enumerate(iterator.sample(3)):\n",
    "        print(f\"  Sample {i+1}: ID={sim_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Fast Generator (for streaming)\n",
    "print(\"=== Fast Generator ===\")\n",
    "try:\n",
    "    count = 0\n",
    "    for sim_id, metadata_vals, h5_path in iter_simulations(data_dir, \"h5\"):\n",
    "        print(f\"ID: {sim_id}, Metadata shape: {metadata_vals.shape}\")\n",
    "        count += 1\n",
    "        if count >= 5:  # Only show first 5\n",
    "            break\n",
    "    print(f\"\\nGenerator can efficiently stream through all simulations\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PyTorch Integration\n",
    "\n",
    "Demonstrate how to use the dataset with PyTorch for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    print(\"=== PyTorch DataLoader Example ===\")\n",
    "    \n",
    "    try:\n",
    "        # Create dataset\n",
    "        dataset = SimulationDataset(data_dir, \"h5\")\n",
    "        \n",
    "        # Create DataLoader\n",
    "        dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=4, \n",
    "            shuffle=True, \n",
    "            num_workers=0  # Set to 0 to avoid multiprocessing issues in Jupyter\n",
    "        )\n",
    "        \n",
    "        print(f\"DataLoader created with batch_size=4\")\n",
    "        print(f\"Number of batches: {len(dataloader)}\")\n",
    "        \n",
    "        # Iterate through first batch\n",
    "        for batch_idx, (sim_ids, metadata_batch, h5_paths) in enumerate(dataloader):\n",
    "            print(f\"\\nBatch {batch_idx + 1}:\")\n",
    "            print(f\"  Simulation IDs: {sim_ids}\")\n",
    "            print(f\"  Metadata batch shape: {metadata_batch.shape}\")\n",
    "            print(f\"  H5 paths: {[p.name for p in h5_paths]}\")\n",
    "            \n",
    "            # Only show first batch\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error with PyTorch DataLoader: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"PyTorch not available - install with: pip install torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Visualization\n",
    "\n",
    "Let's visualize the dataset characteristics and sample simulation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metadata distributions\n",
    "if metadata_file.exists():\n",
    "    metadata = pd.read_csv(metadata_file)\n",
    "    \n",
    "    # Select numeric columns (exclude ID)\n",
    "    numeric_cols = metadata.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'ID' in numeric_cols:\n",
    "        numeric_cols.remove('ID')\n",
    "    \n",
    "    if len(numeric_cols) > 0:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, col in enumerate(numeric_cols[:4]):  # Plot first 4 columns\n",
    "            if i < len(axes):\n",
    "                sns.histplot(metadata[col], ax=axes[i], kde=True)\n",
    "                axes[i].set_title(f'Distribution of {col}')\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(len(numeric_cols), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('Metadata Parameter Distributions', y=1.02, fontsize=16)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No numeric columns found in metadata for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize sample simulation data\n",
    "try:\n",
    "    # Get first simulation\n",
    "    dataset = SimulationDataset(data_dir, \"h5\")\n",
    "    sim_id, metadata_vals, h5_path = dataset[0]\n",
    "    \n",
    "    print(f\"Loading simulation data from: {h5_path.name}\")\n",
    "    \n",
    "    # Load H5 data\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        print(\"H5 file structure:\")\n",
    "        def print_structure(name, obj):\n",
    "            print(f\"  {name}: {type(obj).__name__}\")\n",
    "        f.visititems(print_structure)\n",
    "        \n",
    "        # Try to load some sample data\n",
    "        try:\n",
    "            # This path might need adjustment based on actual H5 structure\n",
    "            data = np.array(f[\"OP10\"][\"blank\"][\"node_displacement\"])\n",
    "            print(f\"\\nLoaded data shape: {data.shape}\")\n",
    "            print(f\"Data type: {data.dtype}\")\n",
    "            print(f\"Data range: [{data.min():.6f}, {data.max():.6f}]\")\n",
    "            \n",
    "            # Visualize if data is reasonable size\n",
    "            if data.size < 10000:  # Only plot if not too large\n",
    "                plt.figure(figsize=(12, 4))\n",
    "                \n",
    "                plt.subplot(1, 2, 1)\n",
    "                if len(data.shape) == 2:\n",
    "                    plt.imshow(data, aspect='auto', cmap='viridis')\n",
    "                    plt.colorbar()\n",
    "                    plt.title('2D Data Heatmap')\n",
    "                else:\n",
    "                    plt.plot(data.flatten()[:1000])  # Plot first 1000 points\n",
    "                    plt.title('Data Values (first 1000 points)')\n",
    "                \n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.hist(data.flatten(), bins=50, alpha=0.7)\n",
    "                plt.title('Data Value Distribution')\n",
    "                plt.xlabel('Value')\n",
    "                plt.ylabel('Frequency')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"Data too large for visualization (>10k elements)\")\n",
    "                \n",
    "        except KeyError as e:\n",
    "            print(f\"Could not access expected data path: {e}\")\n",
    "            print(\"Please check the H5 file structure above and adjust the data path\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error loading simulation data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Recommendations\n",
    "\n",
    "### When to use each access method:\n",
    "\n",
    "1. **SimulationDataset (PyTorch)**: \n",
    "   - ✅ Machine learning training with PyTorch\n",
    "   - ✅ Random access to samples\n",
    "   - ✅ Batch processing with DataLoader\n",
    "   - ❌ Requires PyTorch dependency\n",
    "\n",
    "2. **SimulationIterator (Lightweight)**:\n",
    "   - ✅ No PyTorch dependency\n",
    "   - ✅ Memory efficient streaming\n",
    "   - ✅ Random sampling capability\n",
    "   - ❌ Sequential access only\n",
    "\n",
    "3. **iter_simulations (Generator)**:\n",
    "   - ✅ Ultra-fast streaming\n",
    "   - ✅ Minimal memory footprint\n",
    "   - ✅ Simple function interface\n",
    "   - ❌ No random access or sampling\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Explore the actual H5 file structure for your specific use case\n",
    "- Implement custom data loading for your ML targets\n",
    "- Consider data preprocessing and augmentation strategies\n",
    "- Set up proper train/validation/test splits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}